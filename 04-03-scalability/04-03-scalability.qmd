---
title: "Scalability"
subtitle: "Preparing for Production"
author: Dr Zak Varty
date: ""
editor: source
format:
  revealjs:
    theme: ../eds-slides-theme.scss #(default / dark / simple)
    logo: assets/EDS-logo.jpg
    bibliography: ../refs.bib
    footer: "Effective Data Science: Production - Scalability - Zak Varty"
    menu: true
    slide-number: true
    show-slide-number: all # (all / print / speaker)
    self-contained: true # (set to true before publishing html to web)
    chalkboard: false # (conflicts with self-contained)
      #src: drawings.json
      #theme: whiteboard
      #read-only: true
      #buttons: false
    width: 1600 # default is 1050
    height: 900 # default is 850
    incremental: false
---

## Scalability and Production 

When put into production code gets used more and on more data. 

We will likely have to consider scalability of our methods in

 - Computation time 
 
 - Memory requirements

When doing so we have to balance a trade-off between development costs and usage costs.

## Example: Bayesian Inference

- MCMC originally takes ~24 hours

- Identifying and amending bottlenecks in code reduced this to ~24 minutes. 

_Is this actually better?_

::::{.columns}
:::{.column width="50%"}
- human hours invested
- frequency of use
:::
:::{.column width="50%"}

-  safe / stable / general / readable 

- trade for scalability
:::
::::


## Knowing when to worry 

::::{.columns}
:::{.column width="60%"}

Sub-optimal optimisation can be worse than doing nothing 

> ... programmers have spent far too much time worrying about efficiency in _the wrong places_ and at _the wrong times_; premature optimisation is the root of all evil (or at least most of it) in programming. - Donald Knuth

:::
:::{.column width="40%"}

![](images/pareto-frontier.png){alt="Plot of computation time agains coding time with feasible solutions shown as crosses and the Pareto frontier added as an orange curve."}

:::
::::

:::{.notes}
- Scientific investigation plus some intuition
::: 

## This Lecture

::::{.columns}
:::{.column width="50%"}
- Basic profiling to find bottlenecks. 
- Some simple solutions 
- Strategies for scalable (R) code
- Signpost advanced methods & further reading
:::
::::

## Profiling your code: basics

::::{.columns}
:::{.column width="50%"}
__R as a stopwatch__ 

```{r, echo=TRUE}
t_start <- Sys.time()
Sys.sleep(0.5) # YOUR CODE
t_end <- Sys.time()

t_end - t_start
``` 
<br>

```{r, echo=TRUE}
library(tictoc)

tic() 
Sys.sleep(0.5) # YOUR CODE 
toc()
``` 
::: 
:::{.column width="50%"}

With `{tictoc}` we can get fancy 

```{r, echo=TRUE, collapse=TRUE}
tic("total")
tic("first, easy part")
Sys.sleep(0.5)
toc(log = TRUE)
tic("second, hard part")
Sys.sleep(3)
toc(log = TRUE)
toc()
```
:::
::::

## Profiling your code in detail

To diagnose scaling issues you have to understand what your code is doing.

- Stop the code at time $\tau$ and examine the _call-stack_. 

  - The current function being evaluated, the function that called that, the function that called that, ..., top level function. 

- Do this a lot and you can measure (estimate) the proportion of working memory (RAM) uses over time and the time spent evaluating each function. 

```{r}
library(profvis)
library(bench)
```


## Profiling: Toy Example {.smaller}

::::{.columns}
:::{.column width="40%"}
```{r, echo=TRUE}

h <- function() {
  profvis::pause(1)
}

g <- function() {
  profvis::pause(1)
  h()
}

f <- function() {
  profvis::pause(1)
  g()
  profvis::pause(1)
  h()
}
```
:::
:::{.column width="10%"}
:::
:::{.column width="50%"}
![](images/call-stack.png){alt="schematic diagram of a call stack. x-axis shows time and nested function calls are shown as stacked blocks of varying widths, with the deepest function call at the top. At time tau = 2.5, the pause function is being evaluated within function h, which is being evaluated within function g, which in turn is being evaluated within function f."}
:::
::::

## Profiling: How To
```{r, echo=TRUE, eval=FALSE}
source("assets/prof-vis-example.R")
profvis::profvis(f())
```

![](images/profiling-example-speed.png){alt="RStudio window showing interacting profile of function f. A veritcal bar chart shows how much time is spent evaluating functions f,g,h, and pause."}

## Notes on Time Profiling

- Will get slightly different results each time you run the function 

  - Changes to internal state of computer
  - Usually not a big deal, mainly effects fastest parts of code
  - Be careful with stochastic simulations
  - Use `set.seed()` to make a fair comparison over many runs.


## Notes on Profiling 

::::{.columns}
:::{.column width="50%"}
_Function Source_
```{r}
pad_with_NAs <- function(x, n_left, n_right){
  c(rep(NA, n_left), x, rep(NA, n_right))
}
```

```{r, echo=TRUE}
pad_with_NAs
```
_Compiled Function_
```{r, echo=TRUE}
mean
```
:::
:::{.column width="50%"}
<br>

- Compiled functions have no R source code. 

<br>

- Profiler does not extend into compiled code, see [{jointprof}]( https://github.com/r-prof/jointprof) if you really need this.   

:::
::::



## Memory Profiling 

`profvis()` can similarly measure the memory usage of your code. 

::::{.columns}
:::{.column}
```{r, echo=TRUE}
x <- integer()
for (i in 1:1e4) {
  x <- c(x, i)
}
``` 

![](images/profiling-example-memory.png){alt="RStudio interactive code profiler. Code to iteratively extend a vector uses and clears a lot of working memory. The garbage collector function GC causes most of the runtime."}
:::
:::{.column width="50%"}
- Copy-on-modify behaviour makes growing objects slow.  
- Pre-allocate storage where possible. 
- Strategies and structures, see [R inferno](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf) and [Effecient R](https://csgillespie.github.io/efficientR/performance.html). 
::: 
::::

# Tips to work at scale 

:::{.notes}
TL;DR for all of this is that you should do your loops in a fast language.
:::


## Vectorise 
<br>

::::{.columns}
:::{.column width="50%"}
```{r, echo=TRUE}
x <- 1:10
y <- 11:20 
z <- rep(NA, length(x))

for (i in seq_along(x)) {
  z[i] <- x[i] * y[i]
}
```
:::
:::{.column width="50%"}
```{r echo=TRUE}
x <- 1:10
y <- 11:20 
z <- x * y
```
:::
::::

<br>

Use and write functions with vectorised inputs.

```{r, echo = TRUE, eval=FALSE}
rnorm(n = 100, mean = 1:10, sd = rep(1, 10))
```

Be careful of recycling! 

## Special vectors: Linear Algebra 

<br>

```{r, echo=TRUE}
X <- diag(x = c(2, 0.5))
y <- matrix(data = c(1, 1), ncol = 1)

X %*% y
```
<br>
<br>

More on vectorising: [Noam Ross Blog Post](http://www.noamross.net/archives/2014-04-16-vectorization-in-r-why/)


## For loops in disguise: the apply family {.smaller}

Functional programming equivalent of a for loop. [`apply()`, `mapply()`, `lapply()`, ...]

Apply a function to each element of a list-like object. 

<br>

```{r, echo = TRUE}
A <- matrix(data = 1:12, nrow = 3, ncol = 4)
A
```
<br>

```{r, echo = TRUE}
# MARGIN = 1 => rows,  MARGIN = 2 => columns
apply(X = A, MARGIN = 1, FUN = sum)
```

Generalises functions from `{matrixStats}`

```{r, echo = TRUE}
rowSums(A)
```

## For loops in disguise: purrr::map {.smaller}

<br>

::::{.columns}
:::{.column width="50%"}
__Iterate over a single object with `map()`:__ 

```{r, echo = TRUE}
mu <- c(-10, 0, 10)
purrr::map(.x = mu, .f = rnorm, n = 5)
```
:::
:::{.column width="50%"}
__Iterate over multiple objects `map2()` and `pmap()`:__
```{r, echo = TRUE}
mu <- c(-10, 0, 10)
sigma <- c(0, 0.1, 0)
purrr::map2(.x = mu, .y = sigma, .f = rnorm, n = 5)
```
:::
::::

<br>

For more details and variants see Advanced R [chapters 9-11](https://adv-r.hadley.nz/functionals.html) on functional programming. 

## Easy parallelisation with furrr {.smaller}

<br> <br>

::::{.columns}
:::{.column width="50%"}

- `{parallel}` and `{futures}` allow parallel coding over multiple cores. 

- Powerful, but steep learning curve.

- `{furrr}` makes this very easy, just add `future_` to purrr verbs. 
:::
:::{.column width="50%"}
```{r, echo=TRUE}
mu <- c(-10, 0, 10)
furrr::future_map(
  .x = mu, 
  .f = rnorm,
  .options = furrr::furrr_options(seed = TRUE),
  n = 5) 
```
:::
::::

<br> <br>

Need to be very careful handling RNG. See [R-bloggers](https://www.r-bloggers.com/2020/09/future-1-19-1-making-sure-proper-random-numbers-are-produced-in-parallel-processing/) for more details. 

## Sometimes R doesn't cut it 

::::{.columns}
:::{.column width="50%"}
![](images/Rccp.png){alt="Logo of the Rcpp package. A speed dial turned up to 11 out of 10 in a blue hexagon."}
:::
:::{.column width="50%"}
- An API for running C++ code in R 
  - Loops that need to be run in order 
  - Lots of function calls (e.g. deep recursion)
  - Fast data structures 
  
- Beyond our scope but good to know exists. Starting point: Advanced R [Chapter 25](https://adv-r.hadley.nz/rcpp.html).
:::
::::

## Wrapping up 

::::{.columns}
:::{.column width="50%"}
### Summary
1. Pick you battles wisely
2. Target your energy with profiling
3. Scale loops with vectors
4. Scale loops in parallel processing
5. Scale in another language
:::
:::{.column width="50%"}
### Help!
- Articles and blog links
- The R inferno [(Circles 2-4)](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf) 
- Advanced R [(Chapters 23-25)](https://adv-r.hadley.nz/techniques.html), 
- Efficient R [(Chapter 7)](https://csgillespie.github.io/efficientR/performance.html#prerequisites-6). 
:::
::::
