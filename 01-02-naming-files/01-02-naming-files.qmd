---
title: "Naming Files"
subtitle: "Data Science Workflows"
author: Dr Zak Varty
date: ""
editor: source
format:
  revealjs:
    theme: ../eds-slides-theme.scss #(default / dark / simple)
    logo: assets/EDS-logo.jpg
    bibliography: ../refs.bib
    footer: "Effective Data Science: Workflows - Naming Files - Zak Varty"
    menu: true
    slide-number: true
    show-slide-number: all # (all / print / speaker)
    self-contained: true # (set to true before publishing html to web)
    chalkboard: false # (conflicts with self-contained)
      #src: drawings.json
      #theme: whiteboard
      #read-only: true
      #buttons: false
    width: 1600 # default is 1050
    height: 900 # default is 850
    incremental: false
---

## Naming things is easy, Naming things *well* is hard

<br>

> "There are only two hard things in Computer Science: cache invalidation and naming things."
>
> **Phil Karlton, Netscape Developer**

::: notes
When working on a data science project we can in principle name directories, files, functions and other objects whatever we like. In reality though, using an ad-hoc system of naming is likely to cause confusion, headaches and mistakes. We obviously want to avoid all of those things, in the spirit of being kind to our current colleges and also to our future selves.

Coming up with good names is an art form. Like most art, naming things is an activity that you get better at with practice. Another similarity is that the best naming systems don't come from giving data scientists free reign over their naming system. Like all art, the best approaches to naming things give you strong guidelines and boundaries within which to express your creativity and skill.

In this lecture we'll explore what these boundaries and what we want them to achieve for us. The content of this lecture is based largely around a talk of the same name given by Jennifer Bryan and the tidyverse style guide, which forms the basis of Google's style guide for R programming.
:::

<!-- ======================================================================= -->

# Part 1: Naming Files

::: {.notes}
We'll be begin by focusing in on what we call our files. That is, we'll first focus on the part of the file name that comes before the dot. In the second part of this video, we'll then cycle back around to discuss file extensions. 
:::
<!-- ======================================================================= -->

------------------------------------------------------------------------

## What do we want from file names?

::: columns
::: {.column width="40%"}
<br>

1.  Machine Readable

<br>

2.  Human Readable

<br>

3.  Order Friendly
:::

::: {.column width="60%"}
**Current names:** <br>

    abstract.docx
    Effective Data Science's module guide 2022.docx 
    fig 12.png
    Rplot7.png
    1711.05189.pdf
    HR Protocols 2015 FINAL (Nov 2015).pdf

<br>

**Better names:** <br>

    2015-10-22_human-resources-protocols.pdf
    2022_effective-data-science-module-guide.docx
    2022_RSS-conference-abstract.docx 
    fig12_earthquake-timeseries.png 
    fig07_earthquake-location-map.png
    ogata_1984_spacetime-clustering.pdf
:::
:::

::: {.notes}
Before we dive into naming files, we should first consider what we want from the file names that we choose. There are three key properties that that we would like to satisfy. 

Thee first desirable property is for file names to be easily readable by computers, the second is for the file names to be easily readable by humans and finally the file names should take advantage of the default ordering imposed on our files.  

This set of current file names is sorely lacking across all of these properties, and we want to provide naming conventions to move us toward the better file names listed below. 

Let's take a few minutes to examine what exactly we mean by each of these properties.
:::
------------------------------------------------------------------------

## Machine Readable

What do we mean by machine readable file names?

-   Easy to compute on by *deliberate use of delimiters*:
    -   `underscores_separate_metadata`, `hyphens-separate-words`.

<br>

-   Play nicely with *regular expressions* and *globbing*:
    -   avoid spaces, punctuation, accents, cases;
    -   `rm Rplot*.png`

::: {.notes}
When we are operating on a large number of files it is useful to be able to work with them programmatically. 

One example of where this might be useful is when downloading assessments for marking. This might require me to unzip a large number of zip files, copying the pdf report from each unzipped folder into a single directory and all of the R scripts from each unzipped folder into another directory. The marked scripts and code then need to be paired back up in folders named by student, and re-zipped ready to be returned. 

This is _monotonously_ dull and might work for ~50 students but not for ~5000. Working programmatically with files is the way to get this job done efficiently. This requires the file names to play nicely with the way that computers interpret file names, which they regard as a string of characters. 

It is often helpful to have some metadata included in the file name, for example the student's id number and the assessment title. We will use an underscore to separate elements of metadata within the file name and a hypen to separate sub-elements of meta-data, for example words within the assessment title. 

Regular expressions and globbing are two ideas from string manipulation that you may not have met, but which will inform our naming conventions. Regular expressions allow you to search for strings (in our case file names) that match a particular pattern. Regular expressions can do really complicated searches but become gnarly when you have to worry about special characters like spaces, punctuation, accents and cases, so these should be avoided in file names. 

A special type of regular expression is called globbing where a star is used to replace any number of subsequent characters in a file name, so that here we can delete all png images that begin with Rplot using a single line of code. Globbing becomes particular powerful when you use a consistent structure to create your file names. 
:::
------------------------------------------------------------------------

## Machine Readable

Machine readable names are useful when:

-   *managing files*: ordering, finding, moving, deleting:

<br>

-   *extracting information* directly from file names,

<br>

-   *working programmatically* with file names and regex.

::: {.notes}
As in the assessment marking example, having machine readable file names is particularly useful when managing files, such as ordering, finding, moving or deleting them. Another example of this is when your analysis requires you to load a large number of individual data files. 

Machine readable file names are also useful for extracting meta-information from files without having to open them in memory. This is particularly useful when the files might be too large to load into memory, or you only want to load data from a certain year. 

The final benefit we list here is the scalability, reduction in drudgery and lowered risk for human error when operating on a very large number of files.
:::

## Order Friendly

Start with a number if there is a logical order to your files (e.g steps in an analysis).

::: columns
::: {.column width="45%"}
**Original**

::: smaller
> `diagnositc-plots.R`
>
> `download.R`
>
> `runtime-comparison.R`
>
> `...`
>
> `model-evaluation.R`
>
> `wrangle.R`
:::
:::

::: {.column width="55%"}
**Refined**

::: smaller
> `00_download.R`
>
> `01_wrangle.R`
>
> `02_model.R`
>
> `...`
>
> `09_model-evaluation.R`
>
> `10_model-comparison-plots.R`
:::
:::
:::

::: {.notes}
The next property we will focus on also links to how computers operate. We'd like our file names to exploit the default orderings used by computers. This means starting file names with character strings or metadata that allow us order our files ion some meaningful way. 

One example of this is where there's some logical order in which your code should be executed. Prepreding numbers makes this ordering immediately obvious. Starting single digit numbers with a leading 0 is a very good idea here to prevent script 1 being sorted in with the tens, script 2 in with the twenties and so on. If you might have over 100 files, for example when saving the output from many simulations, use two or more zeros to maintain this nice ordering. 
:::
------------------------------------------------------------------------

## Order Friendly

Start with a date for chronologically ordered files (e.g. data, versions, regular reports)

> `2015-10-22_human-resources-protocols.pdf`
>
> `2022-effective-data-science-module-guide.docx`

The ISO 8601 standard for dates was created for a reason. Use it.

. . .

<h2 class="r-fit-text">

YYYY-MM-DD

</h2>

::: {.notes}
A second example of orderable file names is when the file has a date associated with it. This might be a version of a report or the date on which some data were recorded, cleaned or updated. When using dates, in file names or elsewhere, you should conform to the ISO standard date format. 

*click*

This format uses four numbers for the year, followed by two numbers for the month and two numbers of the day of the month. This structure mirrors a nested file structure moving from least to most specific. It also avoids confusion over the ordering of the date elements. Without using the ISO standard a date like 04-05-22 might be interpreted as the fourth of May 2022, the fifth of April 2022, or the 22nd of May 2004.  
:::
------------------------------------------------------------------------

## Human Readable

File names should be meaningful, informative and

    easilyReadByRealPeople (camelCase)

    EasilyReadByRealPeople (PascalCase)
    
    easily_read_by_real_people (snake_case)

    easily-read-by-real-people (skewer-case)

<br>

Bad news for `untitled31.R`, `FinalreportV8.docx` and `7032-185.txt`.

<br>

Look into slugs (web URLs, not the animals) for further tips.

::: {.notes}
The final property we would like our file names to have is human readability. This requires the names of our files to be meaningful, informative and easily read by real people. 

The first two of these are handled by including appropriate metadata in the file name. The ease with which these are read by real people is determined by the length of the file name and by how that name is formatted. 

There are lots of formatting options with fun names like `camelCase`, `PascalCase`, and `snake_case`. There's weak evidence that suggests snake and skewer cases are most the readable.  We'll use a mixture of these, using snake case _between_ metadata items and skewer case _within_ them. This has a slight cost to legibility, in a tradeoff against making computing on these file names easier. 

The final thing that you have control over is the length of the name. Having short, evocative and useful file names is not easy and is a skill in itself. For some hints and tips you might want to look into tips for writing URL slugs. These are last part of a web address that  are intended to improve accessibility by being immediately and intuitively meaningful to any user. 
:::

## Naming Files - Style Guide Summary

1.  File names should be meaningful, informative and scripts end in `.r`

2.  Stick to letters, numbers underscores (`_`) and hyphens (`-`).

3.  Pay attention to capitalisation `file.r` $\neq$ `File.r` on all operating systems.

4.  Show order with left-padded numbers or ISO dates.

. . .

::: columns
::: {.column width="47%"}
*Original:*

    abstract.docx
    Effective Data Science's module guide 2022.docx 
    fig 12.png
    Rplot7.png
    1711.05189.pdf
    HR Protocols 2015 FINAL (Nov 2015).pdf
:::

::: {.column width="53%"}
*Refined:*

    2015-10-22_human-resources-protocols.pdf
    2022_effective-data-science-module-guide.docx
    2022_RSS-conference-abstract.docx 
    fig12_earthquake-timeseries.png 
    fig07_earthquake-location-map.png
    ogata_1984_spacetime-clustering.pdf
:::
:::

::: {.notes}
To summarise then, in your work as a data scientist you should strive for file names that 
are meaningful and informative to humans.

To make file names easy to compute on the should not include special characters or spaces; stick to lower case letters, numbers, underscores and hyphens. 

Finally, if there is an intrinsic order to your files, show this by adding left-padded numbers or ISO dates.

*click* 

Following these guidelines we can move away from our original, messy file names and toward a more refined and consistent naming system. 
:::


# Part 2: File Extensions and Where You Work

::: {.notes}
Now that we have covered what comes before the dot in our file names, let's flip our focus and consider the _types_ of file that we use in our work as data scientists, and how the types of file we work with can influence our overall workflow.

If you need a break, now is a great time to pause the video and grab a hot beverage before diving back into this guide on naming files.  
:::
------------------------------------------------------------------------

## What comes after the dot?

So far we have focused entirely on what comes before the dot, the file name.

Equally, if not more, important is what comes after the dot, the file extension.

<br>

`2022-10-31_earthquake-data.csv`

`01-download.R`

`2022_annual_report.docx`

<br>

The file extension describes how information is stored in that file and determines what software can be used view or run it.

<!-- This can have huge ramifications for the portability, reproducibility, version control and IDE compatibility of your work. -->
::: {.notes}
So far we have focused entirely on what comes before the dot, that is the file name.

Equally, if not more, important is what comes after the dot, the file extension.

The file extension describes how information is stored in that file and determines the software that can use, view or run that file.

You likely already use file extensions to distinguish between code scripts, written documents, images, and notebook files. We'll now explore the benefits and drawbacks of various file types with respect to several important features. 
:::

## Open Source vs Proprietary File Types

![](images/file-types-image.png){fig-align="center"}

::: {.notes}
The first feature we'll consider is whether the file type is open source, and can be used by anyone without charge, or if specialist software must be paid for in order to interact with those files. 

Here each column represents a different class of file, moving left to right we have example file types for tabluar data, list-like data and text documents. File types closer to the top are open source while those lower down rely on proprietry software, which may or may not require payment. 

To make sure that our work is accessible to as many people as possible we should favour the open source options like csv files over google sheets or excel, JSON files over Matlab data files, and tex or markdown over a word or google doc. 

This usually has a benefit in terms of project longevity and scalability. The open source file types are often somewhat simpler in structure, making them more robust to changes over time less memory intensive. 

To see this, let's take a look inside some data files.
:::

## Inside a CSV file

Commas separate values, one line per record.

<br>

```{r}
#| echo: true
library(readr)
read_file(file = "images/example.csv")
```

<br>

``` csv
Name,Number 
A,1
B,2
C,3
```

::: {.notes}
CSV or comma separated value files are used to store tabular data. 

In tabular data, each row of the data represents one record and each column represents a data value. 

A csv encodes this by having each record on a separate line and using commas to separate values with that record. You can see this by opening a csv file in a text editor such as notepad. 
:::

## Inside a TSV file

Tabs separate values, one line per record.

<br>

```{r}
#| echo: true
library(readr)
read_file(file = "images/example.tsv")
```

<br>

``` tsv
Name    Number 
A   1
B   2
C   3
```
::: {.notes}
TSV or tab separated value files are also used to store tabular data. 

Like in a csv each record is given on a new line but in a tsv tabs rather than commas are used to separate values with each record. This can also be seen by opening a tsv file in a text editor such as notepad.

One thing to note is that tabs are a separate character and are not just multiple spaces. In plain text these can be impossible to tell apart, so most text editors have an option to display tabs differently from repeated spaces, though this is usually not enabled by default. 
:::

## Inside an Excel File

Issues caused by potential for formatting and multiple sheets.

Carrying around a lot more data than is needed. (8.7 KB vs 29B)

<br>

``` xlsx
504b 0304 1400 0600 0800 0000 2100 62ee
9d68 5e01 0000 9004 0000 1300 0802 5b43
6f6e 7465 6e74 5f54 7970 6573 5d2e 786d
6c20 a204 0228 a000 0200 0000 0000 0000
0000 0000 0000 0000 0000 0000 0000 0000
.... .... .... .... .... .... .... ....
0000 0000 0000 0000 ac92 4d4f c330 0c86
ef48 fc87 c8f7 d5dd 9010 424b 7741 48bb
2154 7e80 49dc 0fb5 8da3 241b ddbf 271c
1054 1a83 0347 7fbd 7efc cadb dd3c 8dea
.... .... .... .... .... .... .... ....
```

::: {.notes}
When you open an excel file in a text editor, you will immediately see that this is not a human interpretable file format. Each entry here is a four digit hexadecimal number and there are a lot more of them than we have entries in our small table. 

This is because excel files can carry a lot of additional information that a csv or tsv are not able to, for example cell formatting or having multiple tables (called sheets by excel) stored within a single file. 

This means that excel files take up much more memory because they are carrying a lot more information than is strictly contained within the data itself. 
:::

## Inside a JSON file

Each record is a collection of `key:value` pairs.

Can be nested to store `list`-like or `dictionary`-like objects.

<br>

``` json
[{
    "Name": "A",
    "Number": "1"
}, {
    "Name": "B",
    "Number": "2"
}, {
    "Name": "C",
    "Number": "3"
}]
```
:::{.notes}
JSON, or Java Script Object Notation, files are an open source format for list-like data. Each record is represented by a collection of `key:value` pairs. In our example table each entry has two fields, one corresponding to the `Name` key and one corresponding to the `Number` key.

This list-like structure allows non-tabular data to be stored by using a property called nesting: the value taken by a key can be a single value, a vector of values or another list-like object. 

This ability to create nested data structures has lead to this data format being used widely in a range of applications that require data transfer.
:::

## Inside a XML file {.smaller}

Also a collection of `key:value` pairs, but formatted differently.

<br>

``` html
<?xml version="1.0" encoding="UTF-8"?>
<root>
  <row>
    <Name>A</Name>
    <Number>1</Number>
  </row>
  <row>
    <Name>B</Name>
    <Number>2</Number>
  </row>
  <row>
    <Name>C</Name>
    <Number>3</Number>
  </row>
</root>
```
:::{.notes}
XML files are another open source format for list-like data, where each record is represented by a collection of `key:value` pairs. 

The difference from a JSON file is mainly in how those records are formatted within the file. In a JSON file this is designed to look like objects in the Java Script programming language and in XML the formatting is done to look like html, the markup language used to write websites.
:::


## A note on notebooks

-   There are two and a half notebook formats that you are likely to use to: <br>`.rmd` (alternatively `.qmd`) and `.ipynb`.

-   R markdown documents `.rmd` are plain text files, so are very human friendly.

-   ***JuPy***te***R*** notebooks have multi-language support but are not so human friendly (JSON in disguise).

-   Quarto documents offer the best of both worlds and more extensive language support. Not yet as established as a format.

::: {.notes}
In addition to the files you read and write, the files that you code in will largely determine your workflow. 

There are three main options for the way that you code: first is typing it directly at the command line, second is using a text editor or IDE to write scripts and third is writing a notebook that mixes code. text and output together in a single document. 

We'll compare these methods of working on the next slide, but first let's do a quick review of what notebooks are available to you and why you might want to use them. 

As a data scientist, there are two and a half notebook formats that you' are likely to use're like to have met before. The first two are Rmarkdown files for those working predominantly in R and interactive python or jupyter notebooks for those working predominantly in python. The final half format are quarto markdown documents, which are relatively new and extend the functionality of Rmarkdown files. 

The main benefit of R markdown documents is that they're plain text files, so they're very human friendly. ***JuPy***te***R*** notebooks have the benefit of supporting code written in Julia, Python or R, but are not so human friendly - under the hood these documents are JSON files that should not be edited directly because a misplaced bracket will break them.

Quarto documents offer the best of both worlds, with plain text formatting and even more extensive language support than jupyter notebooks. Quarto is a recent extension of Rmarkdown, which is rapidly becoming popular in the data science community. 

Each format has its benefits and drawbacks depending on the context in which they are used and all have some shared benefits and limitiations by nature of them all being notebook documents. 
::: 

## File extensions and where you code

<br>

| Property              | Notebook | Script | Command Line |
|-----------------------|:--------:|:------:|:------------:|
| reproducible          |    \~    |   ✓    |      X       |
| readable              |    \~    |   ✓    |      \~      |
| self-documenting      |    ✓     |   X    |      X       |
| in production         |    X     |   ✓    |      \~      |
| ordering / automation |    \~    |   ✓    |      \~      |

::: {.notes}
The main benefit of notebook documents is that they are self-documenting, in that they can mix the documentation, code and report all into a single document.  Notebooks also they provide a level of interactivity when coding that is not possible when working directly at the command line or using a text editor to write scripts. This second factor is easily overcome by using an integrated development environment when scripting. 

Writing code in .r files is not self-documenting but this separation of code, documentation and outputs has many other benefits. Firstly, the resulting scripts provide a reproducible and automatable workflow, unlike one-off lines of code being run at the command line. Secondly, using an IDE to write these provides you with syntax highlighting and code linting features to help you write readable and accurate code. Finally, the separation of code from documentation and output allows your work to be more easily or even directly put into production.

In this course we will advocate for a scripting-first approach to data science, though notebooks and command line work definitely have their place. 

Notebooks are great as teaching and rapid development tools but have strong limitations with being put into production. Conversely, coding directly at the command line can leave no trace of your workflow and lead to an analysis that cannot be replicated in the future.
:::

## Summary

<br>

::: columns
::: {.column width="50%"}
**Name files so that they are:**

-   Machine Readable,
-   Human Readable,
-   Order Friendly.
:::

::: {.column width="50%"}
**Use document types that are:**

-   Widely accessible,
-   Easy to read and reproduce,
-   Appropriate for the task at hand.
:::
::: 

::: {.notes}
Finally, let's wrap things up by summarising what we have learned about naming files. 

Before the dot we want to pick file names that machine readable, human friendly and play nicely with the default orderings provided to us. 

After the dot, we want to pick file types that are widely accessible, easily read by humans and allow for our entire analysis to be reproduced.

Above all we want to name our files and pick our file types to best match with the team we are working in and the task that we is at hand. 

That is all for this video, thank you very much for your attention. 
:::
